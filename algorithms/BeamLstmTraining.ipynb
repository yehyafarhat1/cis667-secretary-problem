{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.0306e-09, 5.0000e-01, 5.0000e-01])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tr.tensor([-10, 10, 10])\n",
    "tr.exp(a) / tr.exp(a).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "sentences = [\n",
    "  \"How are you\",\n",
    "  \"Who are you\",\n",
    "  \"Who are they\",\n",
    "  \"Who are we\",\n",
    "  \"Who am I\",\n",
    "  \"Who am I\",\n",
    "  \"Where are you going\"\n",
    "]\n",
    "\n",
    "# replace to training dataset from training datas\n",
    "## 1. all the elements in sentences are integers, elements equal to words in this model\n",
    "\n",
    "## 2. read all elements from training data with randomized segments having same orders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '4', '3', '3', '0', '0', '1', '3', '1', '3', '4', '1', '0', '8', '8', '3', '0', '2', '0', '0', '0', '2', '1', '0', '0', '6', '4', '8', '1', '3', '4', '0', '1', '4', '1', '2', '3', '3', '0', '7', '4', '3', '0', '5', '0', '5', '1', '1', '2', '1', '1', '0', '1', '0', '3', '1', '4', '0', '3', '1', '0', '5', '1', '2', '3', '0', '3', '1', '1', '3', '3', '2', '2', '0', '2', '6', '3', '1', '1', '0', '4', '2', '0', '4', '3', '2', '1', '3', '2', '2', '1', '0', '1', '2', '1', '3', '0', '7', '0', '0', '-1'], ['5', '3', '1', '1', '2', '1', '2', '4', '0', '0', '4', '0', '1', '3', '1', '0', '1', '3', '2', '4', '0', '0', '1', '5', '0', '2', '0', '3', '6', '4', '4', '2', '0', '1', '1', '1', '1', '2', '2', '2', '0', '0', '3', '4', '3', '9', '7', '5', '3', '2', '2', '2', '5', '1', '5', '2', '0', '2', '2', '8', '0', '2', '1', '0', '5', '1', '0', '9', '4', '0', '0', '1', '0', '7', '1', '1', '3', '0', '2', '6', '3', '1', '3', '0', '1', '0', '5', '0', '1', '2', '1', '3', '2', '1', '4', '6', '1', '3', '1', '2', '-1'], ['0', '3', '6', '0', '4', '4', '0', '2', '5', '7', '1', '5', '6', '1', '1', '3', '2', '3', '2', '0', '2', '3', '1', '2', '6', '0', '3', '0', '0', '0', '3', '7', '1', '2', '7', '2', '2', '0', '3', '0', '4', '1', '0', '5', '1', '3', '1', '0', '1', '3', '6', '0', '5', '5', '8', '3', '5', '3', '0', '4', '2', '6', '1', '1', '8', '2', '5', '4', '0', '0', '0', '4', '1', '6', '1', '1', '1', '1', '9', '2', '1', '7', '1', '1', '8', '1', '3', '0', '1', '0', '4', '3', '2', '5', '4', '1', '3', '1', '2', '1', '-1'], ['6', '1', '3', '1', '0', '5', '2', '0', '4', '2', '0', '0', '3', '0', '2', '6', '1', '8', '1', '0', '1', '3', '0', '6', '4', '2', '1', '2', '6', '1', '2', '3', '5', '0', '0', '0', '3', '3', '1', '6', '2', '3', '1', '5', '2', '1', '5', '3', '3', '4', '8', '2', '1', '6', '1', '4', '1', '0', '8', '3', '0', '1', '5', '4', '1', '1', '0', '5', '1', '6', '0', '2', '4', '1', '1', '7', '1', '2', '5', '6', '1', '3', '1', '4', '1', '2', '1', '1', '4', '1', '0', '2', '2', '0', '1', '1', '4', '2', '4', '2', '-1'], ['1', '0', '1', '0', '4', '7', '1', '2', '3', '3', '0', '2', '1', '4', '4', '0', '6', '0', '3', '0', '3', '0', '0', '4', '3', '1', '2', '1', '2', '0', '1', '5', '2', '1', '0', '1', '1', '3', '3', '1', '6', '0', '0', '0', '4', '3', '3', '0', '1', '7', '0', '5', '6', '0', '0', '2', '1', '2', '1', '3', '0', '0', '4', '2', '0', '3', '6', '0', '0', '3', '0', '0', '6', '2', '2', '4', '1', '0', '3', '2', '0', '5', '2', '3', '0', '0', '0', '1', '8', '0', '4', '6', '0', '3', '4', '0', '1', '2', '1', '2', '-1']]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "length:  50\n"
     ]
    }
   ],
   "source": [
    "path1 = \"../data/first_training_set.csv\"\n",
    "path2 = \"../data/second_training_set.csv\"\n",
    "path3 = \"../data/third_training_set.csv\"\n",
    "path4 = \"../data/fourth_training_set.csv\"\n",
    "path5 = \"../data/fifth_training_set.csv\"\n",
    "\n",
    "\n",
    "# pick 5000 (each 1000, imply by the parameter inside) segments in each dataset with randomized segments\n",
    "\n",
    "sentences = []\n",
    "\n",
    "import number_helpers\n",
    "\n",
    "sen1 = number_helpers.augment_sentences(path1, 10, 100)\n",
    "sen2 = number_helpers.augment_sentences(path2, 10, 100)\n",
    "sen3 = number_helpers.augment_sentences(path3, 10, 100)\n",
    "sen4 = number_helpers.augment_sentences(path4, 10, 100)\n",
    "sen5 = number_helpers.augment_sentences(path5, 10, 100)\n",
    "\n",
    "sentences.extend(sen1)\n",
    "sentences.extend(sen2)\n",
    "sentences.extend(sen3)\n",
    "sentences.extend(sen4)\n",
    "sentences.extend(sen5)\n",
    "\n",
    "print(sentences[:5])\n",
    "print('------------------------------------------------------------------------------------------------------------------------')\n",
    "print('length:  '+str(len(sentences)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# start to set building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6', '5', '7', '0', '9')\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make a dictionary mapping each word to a one-hot tensor\n",
    "words = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        #for word in sentence.split(\" \"):\n",
    "        words.add(word)\n",
    "words = tuple(words)  # deterministic order\n",
    "\n",
    "print(words[:5])\n",
    "\n",
    "# PyTorch LSTM expects 3d tensors representing (sequence length, batch size, number of features)\n",
    "I = tr.eye(len(words))\n",
    "dictionary = {\n",
    "    word: I[w].reshape(1, 1, len(words))\n",
    "    for w, word in enumerate(words)}\n",
    "\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# try to train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(11, 3)\n",
      "  (readout): Linear(in_features=3, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a small LSTM recurrent neural network with linear hidden-to-output layer\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = tr.nn.LSTM(input_size=len(words), hidden_size=hidden_size)\n",
    "        self.readout = tr.nn.Linear(in_features=hidden_size, out_features=len(words))\n",
    "\n",
    "    def forward(self, x, v=None):\n",
    "        _, v = self.lstm(x) if v is None else self.lstm(x, v)  # update hidden from input\n",
    "        h, c = v  # LSTM hidden vector and internal so-called \"cell state\"\n",
    "        y = self.readout(h)  # get output from hidden\n",
    "        y = tr.softmax(y, dim=-1)  # make sure output is a probability distribution\n",
    "        return y, v\n",
    "\n",
    "\n",
    "print(Net(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11783.431640625\n",
      "1 11167.8095703125\n",
      "2 10845.189453125\n",
      "3 10580.8095703125\n",
      "4 10344.470703125\n",
      "5 10140.90234375\n",
      "6 9972.6318359375\n",
      "7 9841.626953125\n",
      "8 9749.109375\n",
      "9 9694.251953125\n",
      "10 9672.443359375\n",
      "11 9673.556640625\n",
      "12 9684.4248046875\n",
      "13 9694.8212890625\n",
      "14 9699.79296875\n",
      "15 9697.630859375\n",
      "16 9689.8232421875\n",
      "17 9679.7734375\n",
      "18 9669.59375\n",
      "19 9659.6806640625\n",
      "20 9649.4052734375\n",
      "21 9638.5517578125\n",
      "22 9627.5751953125\n",
      "23 9617.4501953125\n",
      "24 9609.1298828125\n",
      "25 9602.9833984375\n",
      "26 9598.9638671875\n",
      "27 9596.6181640625\n",
      "28 9595.4765625\n",
      "29 9595.1533203125\n",
      "30 9595.296875\n",
      "31 9595.576171875\n",
      "32 9595.775390625\n",
      "33 9595.6220703125\n",
      "34 9595.0458984375\n",
      "35 9594.19140625\n",
      "36 9593.1875\n",
      "37 9592.267578125\n",
      "38 9591.4443359375\n",
      "39 9590.7158203125\n",
      "40 9590.05078125\n",
      "41 9589.2666015625\n",
      "42 9588.447265625\n",
      "43 9587.548828125\n",
      "44 9586.7373046875\n"
     ]
    }
   ],
   "source": [
    "net = Net(3)\n",
    "# opt = tr.optim.SGD(net.parameters(), lr=0.001)\n",
    "opt = tr.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    batch_loss = 0.\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #tokens = sentence.split(\" \")\n",
    "        tokens = sentence\n",
    "        v = None  # no hidden activation at first time-step\n",
    "        for t in range(len(tokens) - 1):\n",
    "            y, v = net(dictionary[tokens[t]], v)\n",
    "            y_target = dictionary[tokens[t + 1]]\n",
    "\n",
    "            #loss = tr.sum((y - y_target)**2) # MSE\n",
    "            loss = -tr.sum(y_target * tr.log(y))  # Cross-entropy\n",
    "            batch_loss += loss\n",
    "\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    print(epoch, batch_loss.item())\n",
    "    # if epoch % 100 == 0: print(epoch, batch_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# complete epoch\n",
    "### and try prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0 0.23600423336029053\n",
      "1 0.23719550669193268\n",
      "0 0.23819178342819214\n"
     ]
    }
   ],
   "source": [
    "# Try predicting\n",
    "word = \"3\"\n",
    "v = None\n",
    "print(word)\n",
    "\n",
    "for t in range(3):\n",
    "  x = dictionary[word]\n",
    "  y, v = net(dictionary[tokens[t]], v)\n",
    "  y = y.squeeze() # ignore singleton dimensions for time-step/example\n",
    "  w = y.argmax()\n",
    "  word = words[w]\n",
    "  prob = y[w]\n",
    "  print(word, prob.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}