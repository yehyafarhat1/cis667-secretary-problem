{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "\n",
    "print(\"CUDA: \"+str(tr.cuda.is_available()))\n",
    "\n",
    "print(\"GPUs: \"+str(tr.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0306e-09, 5.0000e-01, 5.0000e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#syntax  会根据设备情况自动选择cpu或者gpu\n",
    "device = tr.device(\"cuda\" if tr.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "a = tr.tensor([-10, 10, 10])\n",
    "tr.exp(a) / tr.exp(a).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "sentences = [\n",
    "  \"How are you\",\n",
    "  \"Who are you\",\n",
    "  \"Who are they\",\n",
    "  \"Who are we\",\n",
    "  \"Who am I\",\n",
    "  \"Who am I\",\n",
    "  \"Where are you going\"\n",
    "]\n",
    "\n",
    "# replace to training dataset from training datas\n",
    "## 1. all the elements in sentences are integers, elements equal to words in this model\n",
    "\n",
    "## 2. read all elements from training data with randomized segments having same orders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '0', '3', '0', '2', '3', '2', '2', '3', '2', '0', '3', '0', '4', '2', '2', '0', '0', '2', '7', '1', '6', '2', '0', '1', '0', '3', '1', '1', '1', '4', '2', '4', '0', '5', '1', '2', '8', '0', '4', '1', '0', '3', '7', '0', '2', '4', '0', '3', '3', '1', '1', '6', '4', '1', '1', '0', '0', '2', '1', '4', '4', '2', '0', '0', '3', '0', '3', '3', '3', '0', '3', '5', '1', '3', '1', '4', '5', '3', '2', '0', '3', '5', '0', '0', '2', '1', '0', '5', '2', '0', '2', '0', '7', '0', '3', '2', '2', '6', '2', '-1'], ['0', '0', '3', '2', '3', '4', '3', '0', '2', '0', '2', '4', '2', '0', '1', '1', '4', '2', '1', '1', '3', '4', '1', '0', '5', '1', '1', '1', '2', '0', '3', '0', '0', '1', '5', '1', '0', '3', '2', '2', '5', '2', '1', '6', '7', '1', '0', '5', '6', '4', '3', '4', '0', '0', '3', '0', '4', '1', '0', '7', '5', '1', '1', '0', '1', '3', '8', '0', '1', '4', '1', '5', '4', '3', '4', '4', '0', '6', '4', '3', '0', '3', '1', '0', '2', '3', '1', '7', '0', '0', '3', '3', '0', '0', '0', '7', '7', '2', '0', '3', '-1'], ['0', '1', '1', '1', '4', '7', '7', '2', '2', '1', '1', '2', '4', '0', '2', '1', '0', '2', '2', '0', '6', '6', '0', '3', '0', '1', '4', '3', '5', '4', '0', '1', '3', '0', '1', '0', '0', '3', '1', '2', '1', '5', '1', '7', '1', '4', '7', '7', '6', '0', '0', '0', '0', '2', '5', '0', '0', '1', '0', '3', '1', '3', '2', '3', '1', '5', '1', '1', '2', '4', '1', '0', '5', '2', '2', '4', '1', '0', '0', '4', '1', '0', '0', '5', '1', '0', '2', '2', '4', '4', '0', '7', '0', '0', '0', '4', '2', '2', '1', '0', '-1'], ['1', '3', '0', '1', '1', '4', '1', '5', '1', '1', '2', '4', '0', '0', '1', '1', '5', '3', '3', '1', '8', '6', '2', '8', '0', '6', '1', '5', '4', '2', '6', '0', '0', '4', '1', '2', '1', '0', '3', '6', '1', '1', '0', '3', '0', '0', '0', '0', '0', '7', '1', '2', '3', '4', '7', '1', '0', '4', '2', '8', '0', '5', '1', '2', '1', '6', '0', '0', '4', '4', '4', '6', '4', '2', '0', '3', '6', '4', '1', '6', '1', '1', '4', '1', '2', '0', '2', '1', '0', '5', '4', '2', '1', '2', '1', '0', '5', '3', '1', '4', '-1'], ['0', '2', '5', '3', '1', '1', '1', '2', '5', '3', '0', '1', '1', '1', '4', '2', '2', '2', '2', '2', '5', '4', '1', '2', '2', '1', '3', '2', '0', '6', '0', '6', '1', '1', '6', '0', '0', '4', '1', '5', '2', '2', '2', '3', '1', '1', '4', '6', '5', '0', '0', '1', '0', '1', '7', '3', '0', '0', '1', '3', '6', '3', '2', '2', '0', '5', '6', '2', '2', '1', '0', '4', '0', '6', '0', '0', '2', '1', '0', '2', '4', '2', '0', '7', '0', '1', '1', '3', '0', '1', '0', '2', '2', '1', '1', '0', '2', '3', '6', '3', '-1']]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "length:  50\n"
     ]
    }
   ],
   "source": [
    "path1 = \"C:/Users/gzkei/PycharmProjects/cis667-secretary-problem/data/first_training_set.csv\"\n",
    "path2 = \"C:/Users/gzkei/PycharmProjects/cis667-secretary-problem/data/second_training_set.csv\"\n",
    "path3 = \"C:/Users/gzkei/PycharmProjects/cis667-secretary-problem//data/third_training_set.csv\"\n",
    "path4 = \"C:/Users/gzkei/PycharmProjects/cis667-secretary-problem//data/fourth_training_set.csv\"\n",
    "path5 = \"C:/Users/gzkei/PycharmProjects/cis667-secretary-problem//data/fifth_training_set.csv\"\n",
    "\n",
    "\n",
    "# pick 5000 (each 1000, imply by the parameter inside) segments in each dataset with randomized segments\n",
    "\n",
    "sentences = []\n",
    "\n",
    "import number_helpers\n",
    "\n",
    "sen1 = number_helpers.augment_sentences(path1, 10, 100)\n",
    "sen2 = number_helpers.augment_sentences(path2, 10, 100)\n",
    "sen3 = number_helpers.augment_sentences(path3, 10, 100)\n",
    "sen4 = number_helpers.augment_sentences(path4, 10, 100)\n",
    "sen5 = number_helpers.augment_sentences(path5, 10, 100)\n",
    "\n",
    "sentences.extend(sen1)\n",
    "sentences.extend(sen2)\n",
    "sentences.extend(sen3)\n",
    "sentences.extend(sen4)\n",
    "sentences.extend(sen5)\n",
    "\n",
    "print(sentences[:5])\n",
    "print('------------------------------------------------------------------------------------------------------------------------')\n",
    "print('length:  '+str(len(sentences)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# start to set building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3', '4', '5', '0', '8')\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make a dictionary mapping each word to a one-hot tensor\n",
    "words = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        #for word in sentence.split(\" \"):\n",
    "        words.add(word)\n",
    "words = tuple(words)  # deterministic order\n",
    "\n",
    "print(words[:5])\n",
    "\n",
    "# PyTorch LSTM expects 3d tensors representing (sequence length, batch size, number of features)\n",
    "I = tr.eye(len(words))\n",
    "\n",
    "\n",
    "# 将数据放入gpu\n",
    "I = I.to(device) \n",
    "I = I.cuda()\n",
    "\n",
    "dictionary = {\n",
    "    word: I[w].cuda().reshape(1, 1, len(words))\n",
    "    for w, word in enumerate(words)}\n",
    "\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# try to train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(11, 3)\n",
      "  (readout): Linear(in_features=3, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a small LSTM recurrent neural network with linear hidden-to-output layer\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = tr.nn.LSTM(input_size=len(words), hidden_size=hidden_size)\n",
    "        self.readout = tr.nn.Linear(in_features=hidden_size, out_features=len(words))\n",
    "        self.lstm.to(device)\n",
    "        self.readout.to(device)\n",
    "\n",
    "    def forward(self, x, v=None):\n",
    "        _, v = self.lstm(x) if v is None else self.lstm(x, v)  # update hidden from input\n",
    "        h, c = v  # LSTM hidden vector and internal so-called \"cell state\"\n",
    "        y = self.readout(h)  # get output from hidden\n",
    "        y = tr.softmax(y, dim=-1)  # make sure output is a probability distribution\n",
    "        return y, v\n",
    "\n",
    "\n",
    "print(Net(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4562.1787109375\n",
      "1 4484.845703125\n",
      "2 4422.458984375\n",
      "3 4371.78369140625\n",
      "4 4331.923828125\n",
      "5 4300.955078125\n",
      "6 4276.25537109375\n",
      "7 4255.92578125\n",
      "8 4239.154296875\n",
      "9 4225.75048828125\n",
      "10 4215.53076171875\n",
      "11 4207.9931640625\n",
      "12 4202.533203125\n",
      "13 4198.48779296875\n",
      "14 4195.37890625\n",
      "15 4192.90869140625\n",
      "16 4190.85107421875\n",
      "17 4189.11083984375\n",
      "18 4187.6123046875\n",
      "19 4186.30712890625\n",
      "20 4185.154296875\n",
      "21 4184.1357421875\n",
      "22 4183.21826171875\n",
      "23 4182.3974609375\n",
      "24 4181.63037109375\n",
      "25 4180.9541015625\n",
      "26 4180.32763671875\n",
      "27 4179.7509765625\n",
      "28 4179.2265625\n",
      "29 4178.7333984375\n",
      "30 4178.27978515625\n",
      "31 4177.85302734375\n",
      "32 4177.4619140625\n",
      "33 4177.09228515625\n",
      "34 4176.740234375\n",
      "35 4176.42724609375\n",
      "36 4176.11865234375\n",
      "37 4175.8310546875\n",
      "38 4175.5517578125\n",
      "39 4175.29833984375\n",
      "40 4175.05029296875\n",
      "41 4174.83251953125\n",
      "42 4174.611328125\n",
      "43 4174.3955078125\n",
      "44 4174.19775390625\n",
      "45 4174.009765625\n",
      "46 4173.8369140625\n",
      "47 4173.66015625\n",
      "48 4173.49462890625\n",
      "49 4173.33154296875\n",
      "50 4173.19189453125\n",
      "51 4173.04736328125\n",
      "52 4172.90673828125\n",
      "53 4172.771484375\n",
      "54 4172.63720703125\n",
      "55 4172.5224609375\n",
      "56 4172.39501953125\n",
      "57 4172.28662109375\n",
      "58 4172.1708984375\n",
      "59 4172.06640625\n",
      "60 4171.96142578125\n",
      "61 4171.8544921875\n",
      "62 4171.76171875\n",
      "63 4171.65966796875\n",
      "64 4171.57373046875\n",
      "65 4171.48779296875\n",
      "66 4171.39599609375\n",
      "67 4171.3115234375\n",
      "68 4171.2392578125\n",
      "69 4171.15478515625\n",
      "70 4171.07373046875\n",
      "71 4171.00439453125\n",
      "72 4170.919921875\n",
      "73 4170.85400390625\n",
      "74 4170.78857421875\n",
      "75 4170.7294921875\n",
      "76 4170.658203125\n",
      "77 4170.591796875\n",
      "78 4170.529296875\n",
      "79 4170.47216796875\n",
      "80 4170.41552734375\n",
      "81 4170.3544921875\n",
      "82 4170.29931640625\n",
      "83 4170.23779296875\n",
      "84 4170.1953125\n",
      "85 4170.1435546875\n",
      "86 4170.087890625\n",
      "87 4170.0419921875\n",
      "88 4169.98193359375\n",
      "89 4169.93603515625\n",
      "90 4169.89599609375\n",
      "91 4169.85791015625\n",
      "92 4169.810546875\n",
      "93 4169.7607421875\n",
      "94 4169.72314453125\n",
      "95 4169.6796875\n",
      "96 4169.6474609375\n",
      "97 4169.59423828125\n",
      "98 4169.55859375\n",
      "99 4169.52587890625\n",
      "100 4169.4921875\n",
      "101 4169.4560546875\n",
      "102 4169.41845703125\n",
      "103 4169.39013671875\n",
      "104 4169.3505859375\n",
      "105 4169.31787109375\n",
      "106 4169.29248046875\n",
      "107 4169.2490234375\n",
      "108 4169.22802734375\n",
      "109 4169.1953125\n",
      "110 4169.16845703125\n",
      "111 4169.1416015625\n",
      "112 4169.10693359375\n",
      "113 4169.0732421875\n",
      "114 4169.0576171875\n",
      "115 4169.02734375\n",
      "116 4168.99658203125\n",
      "117 4168.962890625\n",
      "118 4168.94580078125\n",
      "119 4168.92333984375\n",
      "120 4168.90625\n",
      "121 4168.88525390625\n",
      "122 4168.85595703125\n",
      "123 4168.83544921875\n",
      "124 4168.80712890625\n",
      "125 4168.7890625\n",
      "126 4168.76953125\n",
      "127 4168.74658203125\n",
      "128 4168.7216796875\n",
      "129 4168.70068359375\n",
      "130 4168.67236328125\n",
      "131 4168.65283203125\n",
      "132 4168.6318359375\n",
      "133 4168.6123046875\n",
      "134 4168.59423828125\n",
      "135 4168.57861328125\n",
      "136 4168.56005859375\n",
      "137 4168.54443359375\n",
      "138 4168.52734375\n",
      "139 4168.517578125\n",
      "140 4168.4970703125\n",
      "141 4168.48046875\n",
      "142 4168.46044921875\n",
      "143 4168.4453125\n",
      "144 4168.43017578125\n",
      "145 4168.40771484375\n",
      "146 4168.39599609375\n",
      "147 4168.3759765625\n",
      "148 4168.36279296875\n",
      "149 4168.3466796875\n",
      "150 4168.33251953125\n",
      "151 4168.3212890625\n",
      "152 4168.3056640625\n",
      "153 4168.28759765625\n",
      "154 4168.2783203125\n",
      "155 4168.26123046875\n",
      "156 4168.25146484375\n",
      "157 4168.23583984375\n",
      "158 4168.22265625\n",
      "159 4168.20458984375\n",
      "160 4168.18798828125\n",
      "161 4168.1806640625\n",
      "162 4168.16748046875\n",
      "163 4168.15478515625\n",
      "164 4168.1396484375\n",
      "165 4168.12548828125\n",
      "166 4168.115234375\n",
      "167 4168.10400390625\n",
      "168 4168.0927734375\n",
      "169 4168.08203125\n",
      "170 4168.06884765625\n",
      "171 4168.05908203125\n",
      "172 4168.04638671875\n",
      "173 4168.033203125\n",
      "174 4168.021484375\n",
      "175 4168.013671875\n",
      "176 4168.0068359375\n",
      "177 4167.99755859375\n",
      "178 4167.9833984375\n",
      "179 4167.96435546875\n",
      "180 4167.9521484375\n",
      "181 4167.95166015625\n",
      "182 4167.94189453125\n",
      "183 4167.9296875\n",
      "184 4167.9130859375\n",
      "185 4167.91162109375\n",
      "186 4167.90283203125\n",
      "187 4167.89111328125\n",
      "188 4167.88232421875\n",
      "189 4167.8720703125\n",
      "190 4167.861328125\n",
      "191 4167.85546875\n",
      "192 4167.84521484375\n",
      "193 4167.8330078125\n",
      "194 4167.8212890625\n",
      "195 4167.81591796875\n",
      "196 4167.80908203125\n",
      "197 4167.80126953125\n",
      "198 4167.796875\n",
      "199 4167.78173828125\n",
      "200 4167.7724609375\n",
      "201 4167.76611328125\n",
      "202 4167.75537109375\n",
      "203 4167.7470703125\n",
      "204 4167.74365234375\n",
      "205 4167.73193359375\n",
      "206 4167.7236328125\n",
      "207 4167.71875\n",
      "208 4167.7158203125\n",
      "209 4167.7041015625\n",
      "210 4167.69091796875\n",
      "211 4167.6826171875\n",
      "212 4167.67431640625\n",
      "213 4167.6728515625\n",
      "214 4167.66455078125\n",
      "215 4167.6552734375\n",
      "216 4167.6484375\n",
      "217 4167.642578125\n",
      "218 4167.6337890625\n",
      "219 4167.62548828125\n",
      "220 4167.62060546875\n",
      "221 4167.61474609375\n",
      "222 4167.6015625\n",
      "223 4167.59521484375\n",
      "224 4167.58935546875\n",
      "225 4167.578125\n",
      "226 4167.56591796875\n",
      "227 4167.56298828125\n",
      "228 4167.55419921875\n",
      "229 4167.55224609375\n",
      "230 4167.54541015625\n",
      "231 4167.53857421875\n",
      "232 4167.5380859375\n",
      "233 4167.5302734375\n",
      "234 4167.5224609375\n",
      "235 4167.51708984375\n",
      "236 4167.50390625\n",
      "237 4167.4990234375\n",
      "238 4167.49658203125\n",
      "239 4167.494140625\n",
      "240 4167.48095703125\n",
      "241 4167.47021484375\n",
      "242 4167.46923828125\n",
      "243 4167.46435546875\n",
      "244 4167.45654296875\n",
      "245 4167.45263671875\n",
      "246 4167.4462890625\n",
      "247 4167.4384765625\n",
      "248 4167.4345703125\n",
      "249 4167.42724609375\n",
      "250 4167.4189453125\n",
      "251 4167.4169921875\n",
      "252 4167.40234375\n",
      "253 4167.39501953125\n",
      "254 4167.39208984375\n",
      "255 4167.38427734375\n",
      "256 4167.37744140625\n",
      "257 4167.36865234375\n",
      "258 4167.35888671875\n",
      "259 4167.35498046875\n",
      "260 4167.3525390625\n",
      "261 4167.34765625\n",
      "262 4167.34326171875\n",
      "263 4167.33447265625\n",
      "264 4167.32373046875\n",
      "265 4167.3251953125\n",
      "266 4167.3193359375\n",
      "267 4167.31005859375\n",
      "268 4167.3046875\n",
      "269 4167.30224609375\n",
      "270 4167.296875\n",
      "271 4167.28857421875\n",
      "272 4167.28271484375\n",
      "273 4167.28271484375\n",
      "274 4167.27783203125\n",
      "275 4167.27490234375\n",
      "276 4167.2646484375\n",
      "277 4167.25830078125\n",
      "278 4167.25146484375\n",
      "279 4167.248046875\n",
      "280 4167.2421875\n",
      "281 4167.23291015625\n",
      "282 4167.22998046875\n",
      "283 4167.228515625\n",
      "284 4167.2236328125\n",
      "285 4167.2216796875\n",
      "286 4167.2177734375\n",
      "287 4167.21044921875\n",
      "288 4167.20361328125\n",
      "289 4167.2001953125\n",
      "290 4167.1962890625\n",
      "291 4167.1904296875\n",
      "292 4167.18408203125\n",
      "293 4167.1748046875\n",
      "294 4167.16845703125\n",
      "295 4167.1640625\n",
      "296 4167.16162109375\n",
      "297 4167.15283203125\n",
      "298 4167.1513671875\n",
      "299 4167.14453125\n",
      "300 4167.13671875\n",
      "301 4167.12890625\n",
      "302 4167.12158203125\n",
      "303 4167.12158203125\n",
      "304 4167.11962890625\n",
      "305 4167.1123046875\n",
      "306 4167.10498046875\n",
      "307 4167.10400390625\n",
      "308 4167.09765625\n",
      "309 4167.09814453125\n",
      "310 4167.09375\n",
      "311 4167.0849609375\n",
      "312 4167.08251953125\n",
      "313 4167.07763671875\n",
      "314 4167.0751953125\n",
      "315 4167.0693359375\n",
      "316 4167.05810546875\n",
      "317 4167.05908203125\n",
      "318 4167.05126953125\n",
      "319 4167.05615234375\n",
      "320 4167.0498046875\n",
      "321 4167.04150390625\n",
      "322 4167.03955078125\n",
      "323 4167.0361328125\n",
      "324 4167.03173828125\n",
      "325 4167.025390625\n",
      "326 4167.021484375\n",
      "327 4167.0185546875\n",
      "328 4167.01416015625\n",
      "329 4166.9990234375\n",
      "330 4166.998046875\n",
      "331 4166.99365234375\n",
      "332 4166.99169921875\n",
      "333 4166.9921875\n",
      "334 4166.9853515625\n",
      "335 4166.97802734375\n",
      "336 4166.96923828125\n",
      "337 4166.96923828125\n",
      "338 4166.96337890625\n",
      "339 4166.9619140625\n",
      "340 4166.95458984375\n",
      "341 4166.95068359375\n",
      "342 4166.94873046875\n",
      "343 4166.94287109375\n",
      "344 4166.94140625\n",
      "345 4166.9375\n",
      "346 4166.93603515625\n",
      "347 4166.92822265625\n",
      "348 4166.92333984375\n",
      "349 4166.9189453125\n",
      "350 4166.91650390625\n",
      "351 4166.908203125\n",
      "352 4166.8984375\n",
      "353 4166.8935546875\n",
      "354 4166.88818359375\n",
      "355 4166.88330078125\n",
      "356 4166.88232421875\n",
      "357 4166.87744140625\n",
      "358 4166.87548828125\n",
      "359 4166.8720703125\n",
      "360 4166.86474609375\n",
      "361 4166.861328125\n",
      "362 4166.85595703125\n",
      "363 4166.8525390625\n",
      "364 4166.8486328125\n",
      "365 4166.84326171875\n",
      "366 4166.84228515625\n",
      "367 4166.8330078125\n",
      "368 4166.83154296875\n",
      "369 4166.82958984375\n",
      "370 4166.818359375\n",
      "371 4166.81591796875\n",
      "372 4166.810546875\n",
      "373 4166.80078125\n",
      "374 4166.80029296875\n",
      "375 4166.79541015625\n",
      "376 4166.79052734375\n",
      "377 4166.78955078125\n",
      "378 4166.78173828125\n",
      "379 4166.78173828125\n",
      "380 4166.7763671875\n",
      "381 4166.7646484375\n",
      "382 4166.76318359375\n",
      "383 4166.7607421875\n",
      "384 4166.7568359375\n",
      "385 4166.75390625\n",
      "386 4166.74853515625\n",
      "387 4166.74560546875\n",
      "388 4166.7470703125\n",
      "389 4166.74267578125\n",
      "390 4166.73828125\n",
      "391 4166.73388671875\n",
      "392 4166.7275390625\n",
      "393 4166.72021484375\n",
      "394 4166.7158203125\n",
      "395 4166.71240234375\n",
      "396 4166.71240234375\n",
      "397 4166.708984375\n",
      "398 4166.703125\n",
      "399 4166.69482421875\n",
      "400 4166.69189453125\n",
      "401 4166.68408203125\n",
      "402 4166.67919921875\n",
      "403 4166.677734375\n",
      "404 4166.671875\n",
      "405 4166.67333984375\n",
      "406 4166.67041015625\n",
      "407 4166.65771484375\n",
      "408 4166.65576171875\n",
      "409 4166.6474609375\n",
      "410 4166.64306640625\n",
      "411 4166.63916015625\n",
      "412 4166.63427734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 4166.63330078125\n",
      "414 4166.62890625\n",
      "415 4166.6259765625\n",
      "416 4166.623046875\n",
      "417 4166.6181640625\n",
      "418 4166.615234375\n",
      "419 4166.61083984375\n",
      "420 4166.607421875\n",
      "421 4166.59912109375\n",
      "422 4166.5986328125\n",
      "423 4166.59326171875\n",
      "424 4166.58251953125\n",
      "425 4166.57958984375\n",
      "426 4166.57763671875\n",
      "427 4166.57763671875\n",
      "428 4166.57373046875\n",
      "429 4166.56640625\n",
      "430 4166.5654296875\n",
      "431 4166.56298828125\n",
      "432 4166.55712890625\n",
      "433 4166.5498046875\n",
      "434 4166.54296875\n",
      "435 4166.54443359375\n",
      "436 4166.5390625\n",
      "437 4166.5341796875\n",
      "438 4166.52490234375\n",
      "439 4166.521484375\n",
      "440 4166.51708984375\n",
      "441 4166.51220703125\n",
      "442 4166.51123046875\n",
      "443 4166.5029296875\n",
      "444 4166.50048828125\n",
      "445 4166.49365234375\n",
      "446 4166.4853515625\n",
      "447 4166.48681640625\n",
      "448 4166.48193359375\n",
      "449 4166.4794921875\n",
      "450 4166.4765625\n",
      "451 4166.47216796875\n",
      "452 4166.470703125\n",
      "453 4166.46142578125\n",
      "454 4166.45458984375\n",
      "455 4166.4482421875\n",
      "456 4166.4501953125\n",
      "457 4166.443359375\n",
      "458 4166.44140625\n",
      "459 4166.4384765625\n",
      "460 4166.435546875\n",
      "461 4166.42529296875\n",
      "462 4166.42431640625\n",
      "463 4166.42041015625\n",
      "464 4166.4130859375\n",
      "465 4166.40869140625\n",
      "466 4166.40673828125\n",
      "467 4166.39794921875\n",
      "468 4166.3935546875\n",
      "469 4166.39111328125\n",
      "470 4166.39404296875\n",
      "471 4166.38427734375\n",
      "472 4166.375\n",
      "473 4166.3740234375\n",
      "474 4166.36767578125\n",
      "475 4166.3681640625\n",
      "476 4166.36279296875\n",
      "477 4166.35791015625\n",
      "478 4166.35205078125\n",
      "479 4166.3447265625\n",
      "480 4166.34814453125\n",
      "481 4166.3447265625\n",
      "482 4166.34130859375\n",
      "483 4166.33447265625\n",
      "484 4166.330078125\n",
      "485 4166.32421875\n",
      "486 4166.3154296875\n",
      "487 4166.314453125\n",
      "488 4166.310546875\n",
      "489 4166.3056640625\n",
      "490 4166.30322265625\n",
      "491 4166.29638671875\n",
      "492 4166.29443359375\n",
      "493 4166.28564453125\n",
      "494 4166.28173828125\n",
      "495 4166.27587890625\n",
      "496 4166.275390625\n",
      "497 4166.27392578125\n",
      "498 4166.2666015625\n",
      "499 4166.26171875\n",
      "500 4166.25537109375\n",
      "501 4166.24853515625\n",
      "502 4166.244140625\n",
      "503 4166.24267578125\n",
      "504 4166.2373046875\n",
      "505 4166.23388671875\n",
      "506 4166.23046875\n",
      "507 4166.2216796875\n",
      "508 4166.2099609375\n",
      "509 4166.208984375\n",
      "510 4166.20703125\n",
      "511 4166.201171875\n",
      "512 4166.201171875\n",
      "513 4166.19921875\n",
      "514 4166.1923828125\n",
      "515 4166.18896484375\n",
      "516 4166.1826171875\n",
      "517 4166.177734375\n",
      "518 4166.177734375\n",
      "519 4166.1767578125\n",
      "520 4166.17822265625\n",
      "521 4166.16259765625\n",
      "522 4166.16162109375\n",
      "523 4166.1484375\n",
      "524 4166.14453125\n",
      "525 4166.1455078125\n",
      "526 4166.1435546875\n",
      "527 4166.1357421875\n",
      "528 4166.12646484375\n",
      "529 4166.1220703125\n",
      "530 4166.1171875\n",
      "531 4166.115234375\n",
      "532 4166.10791015625\n",
      "533 4166.1064453125\n",
      "534 4166.1044921875\n",
      "535 4166.09619140625\n",
      "536 4166.08935546875\n",
      "537 4166.08544921875\n",
      "538 4166.07421875\n",
      "539 4166.0703125\n",
      "540 4166.07275390625\n",
      "541 4166.06640625\n",
      "542 4166.06640625\n",
      "543 4166.0576171875\n",
      "544 4166.05419921875\n",
      "545 4166.0498046875\n",
      "546 4166.04296875\n",
      "547 4166.03955078125\n",
      "548 4166.03857421875\n",
      "549 4166.02880859375\n",
      "550 4166.015625\n",
      "551 4166.00927734375\n",
      "552 4166.00732421875\n",
      "553 4166.005859375\n",
      "554 4166.00048828125\n",
      "555 4166.0\n",
      "556 4165.99267578125\n",
      "557 4165.9892578125\n",
      "558 4165.97998046875\n",
      "559 4165.97314453125\n",
      "560 4165.9697265625\n",
      "561 4165.96826171875\n",
      "562 4165.96630859375\n",
      "563 4165.95751953125\n",
      "564 4165.9521484375\n",
      "565 4165.95166015625\n",
      "566 4165.94677734375\n",
      "567 4165.94580078125\n",
      "568 4165.94287109375\n",
      "569 4165.935546875\n",
      "570 4165.9345703125\n",
      "571 4165.92724609375\n",
      "572 4165.91552734375\n",
      "573 4165.908203125\n",
      "574 4165.9072265625\n",
      "575 4165.90380859375\n",
      "576 4165.90185546875\n",
      "577 4165.890625\n",
      "578 4165.888671875\n",
      "579 4165.8837890625\n",
      "580 4165.880859375\n",
      "581 4165.87451171875\n",
      "582 4165.8671875\n",
      "583 4165.87109375\n",
      "584 4165.8583984375\n",
      "585 4165.853515625\n",
      "586 4165.85009765625\n",
      "587 4165.8427734375\n",
      "588 4165.837890625\n",
      "589 4165.83154296875\n",
      "590 4165.82470703125\n",
      "591 4165.82470703125\n",
      "592 4165.82421875\n",
      "593 4165.818359375\n",
      "594 4165.8134765625\n",
      "595 4165.80419921875\n",
      "596 4165.798828125\n",
      "597 4165.79443359375\n",
      "598 4165.78515625\n",
      "599 4165.78173828125\n",
      "600 4165.77978515625\n",
      "601 4165.7724609375\n",
      "602 4165.76806640625\n",
      "603 4165.7607421875\n",
      "604 4165.75146484375\n",
      "605 4165.75\n",
      "606 4165.74658203125\n",
      "607 4165.74267578125\n",
      "608 4165.7353515625\n",
      "609 4165.724609375\n",
      "610 4165.7177734375\n",
      "611 4165.71923828125\n",
      "612 4165.71240234375\n",
      "613 4165.7158203125\n",
      "614 4165.7138671875\n",
      "615 4165.70361328125\n",
      "616 4165.7001953125\n",
      "617 4165.693359375\n",
      "618 4165.68701171875\n",
      "619 4165.68408203125\n",
      "620 4165.6796875\n",
      "621 4165.67333984375\n",
      "622 4165.662109375\n",
      "623 4165.65625\n",
      "624 4165.65283203125\n",
      "625 4165.646484375\n",
      "626 4165.63720703125\n",
      "627 4165.63916015625\n",
      "628 4165.62646484375\n",
      "629 4165.6240234375\n",
      "630 4165.615234375\n",
      "631 4165.6103515625\n",
      "632 4165.60693359375\n",
      "633 4165.599609375\n",
      "634 4165.59326171875\n",
      "635 4165.59326171875\n",
      "636 4165.58984375\n",
      "637 4165.58544921875\n",
      "638 4165.5751953125\n",
      "639 4165.56787109375\n",
      "640 4165.56201171875\n",
      "641 4165.55712890625\n",
      "642 4165.548828125\n",
      "643 4165.5439453125\n",
      "644 4165.541015625\n",
      "645 4165.53466796875\n",
      "646 4165.5302734375\n",
      "647 4165.52294921875\n",
      "648 4165.517578125\n",
      "649 4165.51708984375\n",
      "650 4165.5166015625\n",
      "651 4165.51513671875\n",
      "652 4165.50732421875\n",
      "653 4165.5009765625\n",
      "654 4165.4921875\n",
      "655 4165.49169921875\n",
      "656 4165.4873046875\n",
      "657 4165.47900390625\n",
      "658 4165.474609375\n",
      "659 4165.466796875\n",
      "660 4165.46142578125\n",
      "661 4165.45751953125\n",
      "662 4165.447265625\n",
      "663 4165.44091796875\n",
      "664 4165.43212890625\n",
      "665 4165.4306640625\n",
      "666 4165.4248046875\n",
      "667 4165.4208984375\n",
      "668 4165.41796875\n",
      "669 4165.41015625\n",
      "670 4165.4033203125\n",
      "671 4165.40087890625\n",
      "672 4165.3935546875\n",
      "673 4165.384765625\n",
      "674 4165.3828125\n",
      "675 4165.37548828125\n",
      "676 4165.3681640625\n",
      "677 4165.36328125\n",
      "678 4165.3623046875\n",
      "679 4165.3486328125\n",
      "680 4165.34326171875\n",
      "681 4165.33642578125\n",
      "682 4165.3369140625\n",
      "683 4165.32666015625\n",
      "684 4165.32470703125\n",
      "685 4165.314453125\n",
      "686 4165.30517578125\n",
      "687 4165.30126953125\n",
      "688 4165.2939453125\n",
      "689 4165.2919921875\n",
      "690 4165.2890625\n",
      "691 4165.27587890625\n",
      "692 4165.27001953125\n",
      "693 4165.26220703125\n",
      "694 4165.25732421875\n",
      "695 4165.2568359375\n",
      "696 4165.24951171875\n",
      "697 4165.23974609375\n",
      "698 4165.23583984375\n",
      "699 4165.23486328125\n",
      "700 4165.22802734375\n",
      "701 4165.21630859375\n",
      "702 4165.20849609375\n",
      "703 4165.2041015625\n",
      "704 4165.20556640625\n",
      "705 4165.197265625\n",
      "706 4165.18798828125\n",
      "707 4165.18359375\n",
      "708 4165.181640625\n",
      "709 4165.17236328125\n",
      "710 4165.1669921875\n",
      "711 4165.16162109375\n",
      "712 4165.15673828125\n",
      "713 4165.14404296875\n",
      "714 4165.1376953125\n",
      "715 4165.13427734375\n",
      "716 4165.12451171875\n",
      "717 4165.11767578125\n",
      "718 4165.11181640625\n",
      "719 4165.1025390625\n",
      "720 4165.10009765625\n",
      "721 4165.099609375\n",
      "722 4165.09521484375\n",
      "723 4165.08935546875\n",
      "724 4165.0751953125\n",
      "725 4165.0703125\n",
      "726 4165.0673828125\n",
      "727 4165.0595703125\n",
      "728 4165.0556640625\n",
      "729 4165.044921875\n",
      "730 4165.03564453125\n",
      "731 4165.03369140625\n",
      "732 4165.02294921875\n",
      "733 4165.0166015625\n",
      "734 4165.01171875\n",
      "735 4165.0107421875\n",
      "736 4165.00341796875\n",
      "737 4164.9921875\n",
      "738 4164.99609375\n",
      "739 4164.97802734375\n",
      "740 4164.97412109375\n",
      "741 4164.9697265625\n",
      "742 4164.95849609375\n",
      "743 4164.95458984375\n",
      "744 4164.9501953125\n",
      "745 4164.9453125\n",
      "746 4164.93505859375\n",
      "747 4164.9267578125\n",
      "748 4164.927734375\n",
      "749 4164.91552734375\n",
      "750 4164.90380859375\n",
      "751 4164.89501953125\n",
      "752 4164.89208984375\n",
      "753 4164.89208984375\n",
      "754 4164.88818359375\n",
      "755 4164.87939453125\n",
      "756 4164.8740234375\n",
      "757 4164.86669921875\n",
      "758 4164.8583984375\n",
      "759 4164.853515625\n",
      "760 4164.849609375\n",
      "761 4164.84326171875\n",
      "762 4164.8310546875\n",
      "763 4164.8251953125\n",
      "764 4164.8212890625\n",
      "765 4164.8115234375\n",
      "766 4164.80712890625\n",
      "767 4164.798828125\n",
      "768 4164.7939453125\n",
      "769 4164.794921875\n",
      "770 4164.78662109375\n",
      "771 4164.78173828125\n",
      "772 4164.77490234375\n",
      "773 4164.76513671875\n",
      "774 4164.7587890625\n",
      "775 4164.75048828125\n",
      "776 4164.74658203125\n",
      "777 4164.73291015625\n",
      "778 4164.73291015625\n",
      "779 4164.7265625\n",
      "780 4164.716796875\n"
     ]
    }
   ],
   "source": [
    "net = Net(3)\n",
    "# syntax 1\n",
    "net.to(device)\n",
    "net.cuda()\n",
    "opt = tr.optim.SGD(net.parameters(), lr=0.001)\n",
    "#opt = tr.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(2000):\n",
    "\n",
    "    batch_loss = 0.\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #tokens = sentence.split(\" \")\n",
    "        tokens = sentence\n",
    "        v = None  # no hidden activation at first time-step\n",
    "        for t in range(len(tokens) - 1):\n",
    "            y, v = net(dictionary[tokens[t]], v)\n",
    "            y_target = dictionary[tokens[t + 1]]\n",
    "\n",
    "            loss = tr.sum((y - y_target)**2) # MSE\n",
    "            #loss = -tr.sum(y_target * tr.log(y))  # Cross-entropy\n",
    "            batch_loss += loss\n",
    "\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    print(epoch, batch_loss.item())\n",
    "    # if epoch % 100 == 0: print(epoch, batch_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# complete epoch\n",
    "### and try prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0 0.23600423336029053\n",
      "1 0.23719550669193268\n",
      "0 0.23819178342819214\n"
     ]
    }
   ],
   "source": [
    "# Try predicting\n",
    "word = \"3\"\n",
    "v = None\n",
    "print(word)\n",
    "\n",
    "for t in range(3):\n",
    "  x = dictionary[word]\n",
    "  y, v = net(dictionary[tokens[t]], v)\n",
    "  y = y.squeeze() # ignore singleton dimensions for time-step/example\n",
    "  w = y.argmax()\n",
    "  word = words[w]\n",
    "  prob = y[w]\n",
    "  print(word, prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '5', '7', '4', '3', '2']\n",
      "x=tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0178, 0.0864, 0.0396, 0.0309, 0.1095, 0.1794, 0.1890, 0.0123, 0.0140,\n",
      "        0.2781, 0.0430], grad_fn=<SqueezeBackward0>), v=(tensor([[[ 0.2002, -0.2487, -0.2157]]], grad_fn=<StackBackward0>), tensor([[[ 0.3453, -0.3728, -0.3543]]], grad_fn=<StackBackward0>)), w=9\n",
      "----------------------------------------\n",
      "word=0 + w=9 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "0 0.2781238257884979\n",
      "----------------------------------------\n",
      "x=tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0206, 0.0698, 0.0406, 0.0361, 0.1839, 0.2115, 0.1879, 0.0144, 0.0153,\n",
      "        0.1761, 0.0439], grad_fn=<SqueezeBackward0>), v=(tensor([[[-0.0499, -0.3574,  0.1078]]], grad_fn=<StackBackward0>), tensor([[[-0.1329, -0.7479,  0.1404]]], grad_fn=<StackBackward0>)), w=5\n",
      "----------------------------------------\n",
      "word=2 + w=5 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "2 0.21149331331253052\n",
      "----------------------------------------\n",
      "x=tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0195, 0.0825, 0.0400, 0.0344, 0.1431, 0.1852, 0.1928, 0.0135, 0.0150,\n",
      "        0.2295, 0.0447], grad_fn=<SqueezeBackward0>), v=(tensor([[[ 0.0476, -0.2662, -0.0850]]], grad_fn=<StackBackward0>), tensor([[[ 0.0854, -0.7636, -0.1196]]], grad_fn=<StackBackward0>)), w=9\n",
      "----------------------------------------\n",
      "word=0 + w=9 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "0 0.22946733236312866\n",
      "----------------------------------------\n",
      "x=tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0141, 0.1076, 0.0407, 0.0344, 0.1226, 0.1793, 0.2196, 0.0081, 0.0114,\n",
      "        0.2094, 0.0528], grad_fn=<SqueezeBackward0>), v=(tensor([[[-0.0676, -0.4690, -0.3296]]], grad_fn=<StackBackward0>), tensor([[[-0.1129, -0.8451, -0.6217]]], grad_fn=<StackBackward0>)), w=6\n",
      "----------------------------------------\n",
      "word=1 + w=6 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "1 0.21959426999092102\n",
      "----------------------------------------\n",
      "x=tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0126, 0.0981, 0.0413, 0.0273, 0.0806, 0.2016, 0.2020, 0.0074, 0.0101,\n",
      "        0.2728, 0.0462], grad_fn=<SqueezeBackward0>), v=(tensor([[[ 0.2771, -0.5115, -0.4299]]], grad_fn=<StackBackward0>), tensor([[[ 0.4273, -1.0363, -0.7588]]], grad_fn=<StackBackward0>)), w=9\n",
      "----------------------------------------\n",
      "word=0 + w=9 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "0 0.2727718651294708\n",
      "----------------------------------------\n",
      "x=tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n",
      "----------------------------------------\n",
      "y=tensor([0.0144, 0.0922, 0.0418, 0.0300, 0.1025, 0.2073, 0.2029, 0.0088, 0.0113,\n",
      "        0.2421, 0.0468], grad_fn=<SqueezeBackward0>), v=(tensor([[[ 0.1745, -0.4851, -0.2944]]], grad_fn=<StackBackward0>), tensor([[[ 0.2384, -1.2467, -0.4762]]], grad_fn=<StackBackward0>)), w=9\n",
      "----------------------------------------\n",
      "word=0 + w=9 ++ ('8', '3', '6', '7', '4', '2', '1', '9', '-1', '0', '5')\n",
      "----------------------------------------\n",
      "0 0.2421417534351349\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# real prediction\n",
    "current_sentence = ['3','5','7','4','3','2']\n",
    "v = None\n",
    "print(current_sentence)\n",
    "for c in current_sentence:\n",
    "    x = dictionary[c]\n",
    "    \n",
    "    print('x={x}'.format(x=x))\n",
    "    print('----------------------------------------')\n",
    "\n",
    "    y, v = net(dictionary[c], v)\n",
    "\n",
    "    y = y.squeeze() # ignore singleton dimensions for time-step/example\n",
    "\n",
    "    w = y.argmax()\n",
    "\n",
    "    print('y={y}, v={v}, w={w}'.format(y=y,v=v,w=w))\n",
    "    print('----------------------------------------')\n",
    "\n",
    "    word = words[w]\n",
    "    print('word={word} + w={w} ++ {words}'.format(word=word,w=w,words=words))\n",
    "    print('----------------------------------------')\n",
    "    prob = y[w]\n",
    "    print(word, prob.item())\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish dumping\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('beanlstm20211217.pkl', 'wb') as handle:\n",
    "    pickle.dump(net, handle)  # like_people is a list with data\n",
    "    input('Finish dumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
